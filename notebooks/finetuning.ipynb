{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69b3659",
   "metadata": {},
   "source": [
    "### Hands-On Tutorial: Fine-Tuning an LLM on Financial Question Answering\n",
    "\n",
    "In this notebook, we’ll walk through a **complete, end-to-end example** of fine-tuning an open-source LLM on a small set of **financial questions and answers**.\n",
    "\n",
    "We will:\n",
    "- **Install and import libraries** for transformer models, datasets, LoRA, and 4-bit quantization (QLoRA).\n",
    "- **Load a 7B LLM (e.g., Mistral 7B) in 4-bit mode** using `bitsandbytes`.\n",
    "- **Load the OpenFinAL `Financial_Question_Answering` dataset** of question–answer pairs ([dataset page](https://huggingface.co/datasets/OpenFinAL/Financial_Question_Answering)).\n",
    "- **Tokenize and format the data** for causal language modeling with a simple QA prompt format.\n",
    "- **Configure and attach LoRA adapters** to the model for parameter-efficient fine-tuning.\n",
    "- **Fine-tune the model** using the Hugging Face `Trainer` API.\n",
    "- **Evaluate before vs. after fine-tuning** on some sample financial questions.\n",
    "\n",
    "This setup is designed for a **single GPU environment (e.g., Google Colab T4, 16 GB VRAM)** using QLoRA so the 7B model fits comfortably into memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9685646",
   "metadata": {},
   "source": [
    "### 1. Environment Setup\n",
    "\n",
    "In this section we:\n",
    "- **Install required libraries** (if running in an environment like Google Colab).\n",
    "- **Import key classes and functions** from Hugging Face and PEFT.\n",
    "- **Detect the available device** (GPU vs CPU).\n",
    "\n",
    "We rely on the following Python packages:\n",
    "- **`transformers`** and **`accelerate`** for model loading and training utilities.\n",
    "- **`datasets`** for handling our small FAQ dataset.\n",
    "- **`peft`** for LoRA configuration and adapters.\n",
    "- **`bitsandbytes`** for 4-bit (QLoRA-style) quantization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee2a7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in an environment like Google Colab, install dependencies.\n",
    "# Comment this out if you already have these packages installed.\n",
    "%pip install -qU transformers accelerate datasets peft bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b44aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "if device != \"cuda\":\n",
    "    print(\"WARNING: Running without a GPU may be very slow or may not fit a 7B model in memory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70843a2c",
   "metadata": {},
   "source": [
    "### 2. Load the Pretrained Model in 4-bit Mode (QLoRA)\n",
    "\n",
    "We now load a **7B-parameter base model** in **4-bit quantized mode** using `bitsandbytes` and the `BitsAndBytesConfig` utility.\n",
    "\n",
    "Key ideas:\n",
    "- **`load_in_4bit=True`** stores model weights in 4-bit precision to save memory.\n",
    "- **`bnb_4bit_quant_type=\"nf4\"`** uses NormalFloat4, a 4-bit datatype shown to preserve accuracy well in QLoRA.\n",
    "- **`bnb_4bit_compute_dtype=torch.bfloat16`** uses bfloat16 for matrix multiplications to reduce quantization error.\n",
    "- **`device_map=\"auto\"`** lets Transformers place layers across available devices (typically your GPU).\n",
    "\n",
    "We also:\n",
    "- Load the **tokenizer** and set its **padding token** to `eos_token` if not already set.\n",
    "- Run a **small padding test** to confirm tokenization and attention masks look as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d0f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"  # 7B parameter base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bedf7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Set padding token if not already set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    print(\"Setting pad token to eos token\")\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "776df26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,       # use double quantization for stability\n",
    "    bnb_4bit_quant_type=\"nf4\",            # NormalFloat4, recommended 4-bit datatype\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # use bfloat16 for computations\n",
    ")\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # let HF allocate layers across GPU (and CPU if needed)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecc584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick tokenizer and padding test\n",
    "texts = [\"Hello world!\", \"This is a slightly longer sentence.\"]\n",
    "enc = tokenizer(texts, padding=True, truncation=True, max_length=10, return_tensors=\"pt\")\n",
    "print(\"Token IDs:\\n\", enc[\"input_ids\"])\n",
    "print(\"Attention masks:\\n\", enc[\"attention_mask\"])\n",
    "print(\"Pad token id:\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4015a3f",
   "metadata": {},
   "source": [
    "### 3. Create and Tokenize the Fine-Tuning Dataset\n",
    "\n",
    "We now use a **simple financial QA dataset from Hugging Face**:\n",
    "\n",
    "- Dataset: **`OpenFinAL/Financial_Question_Answering`**  \n",
    "- Fields: each row has a `Question` and an `Answer` ([dataset page](https://huggingface.co/datasets/OpenFinAL/Financial_Question_Answering)).\n",
    "\n",
    "For this tutorial:\n",
    "- We load the `train` split of the dataset.\n",
    "- We optionally take a **subset** of examples to keep training fast.\n",
    "- We treat the **question as input** and the **answer as the target text**.\n",
    "- For each example we build a simple prompt:\n",
    "  - `Question: <question>\\nAnswer:`\n",
    "- We then tokenize **`prompt + answer`** into a single sequence and use the same token IDs as **labels**, so the model learns to continue the prompt with the answer.\n",
    "\n",
    "This is standard causal language model fine-tuning for QA: the model sees the question and learns to generate the answer that follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89938e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the financial QA dataset from Hugging Face\n",
    "raw_qa = load_dataset(\"OpenFinAL/Financial_Question_Answering\", split=\"train\")\n",
    "\n",
    "print(\"Raw financial QA split size:\", len(raw_qa))\n",
    "print(\"Example entry:\", raw_qa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520574b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw dataset has columns 'Question' and 'Answer'.\n",
    "# Rename them to lowercase for convenience.\n",
    "dataset = raw_qa.rename_columns({\"Question\": \"question\", \"Answer\": \"answer\"})\n",
    "\n",
    "print(\"Total examples in dataset:\", len(dataset))\n",
    "print(\"Example 1:\", {k: dataset[0][k] for k in [\"question\", \"answer\"]})\n",
    "\n",
    "# Split into train and eval splits for formal evaluation\n",
    "splits = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = splits[\"train\"]\n",
    "eval_dataset = splits[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Eval size:\", len(eval_dataset))\n",
    "\n",
    "# We'll reuse an eval example later for qualitative before/after comparison\n",
    "eval_example = eval_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742cab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "\n",
    "\n",
    "def preprocess_function(example):\n",
    "    \"\"\"Format a simple QA prompt and tokenize it for causal LM fine-tuning.\n",
    "\n",
    "    We use the pattern: \"Question: <question>\\nAnswer: <answer>\" and train the\n",
    "    model to predict every token in this sequence (standard next-token\n",
    "    prediction objective).\n",
    "    \"\"\"\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    prompt = f\"Question: {question}\\nAnswer:\"\n",
    "    full_text = prompt + \" \" + answer\n",
    "\n",
    "    enc = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "    return enc\n",
    "\n",
    "\n",
    "# Apply preprocessing to train and eval splits separately\n",
    "tokenized_train = train_dataset.map(preprocess_function, remove_columns=train_dataset.column_names)\n",
    "tokenized_eval = eval_dataset.map(preprocess_function, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "# Set the dataset format for PyTorch\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_eval.set_format(\"torch\")\n",
    "\n",
    "print(\"Tokenized train fields:\", tokenized_train.column_names)\n",
    "print(\"Tokenized eval fields:\", tokenized_eval.column_names)\n",
    "print(\"Example tokenized train input_ids (first 20 tokens):\", tokenized_train[0][\"input_ids\"][:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12731b88",
   "metadata": {},
   "source": [
    "### 4. Configure LoRA and Attach to the Model\n",
    "\n",
    "We will now configure **LoRA (Low-Rank Adapters)** to fine-tune only a **small subset of parameters**:\n",
    "\n",
    "- **`r` (rank)**: size of the low-rank update matrices (we use `r=16`).\n",
    "- **`lora_alpha`**: scaling factor for the LoRA updates (we use `32`, i.e., 2× rank).\n",
    "- **`lora_dropout`**: small dropout for regularization.\n",
    "- **`target_modules`**: which sub-modules to apply LoRA to; for Mistral/LLaMA-style models we often use `\"q_proj\"` and `\"v_proj\"`.\n",
    "\n",
    "Using `get_peft_model` wraps our base model with these adapters. We’ll:\n",
    "- Print the **number of trainable parameters** vs total.\n",
    "- Do a **baseline generation** on one question before any training to see the model’s pre-fine-tuning behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",  # do not update bias terms\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable vs total parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline generation before fine-tuning using one financial QA example\n",
    "eval_question = eval_example[\"question\"]\n",
    "\n",
    "prompt = f\"Question: {eval_question}\\nAnswer:\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "\n",
    "base_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Model output before fine-tuning on this example:\")\n",
    "print(base_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8b492",
   "metadata": {},
   "source": [
    "### 5. Fine-Tune the Model with `Trainer`\n",
    "\n",
    "We now fine-tune the model using the **Hugging Face `Trainer` API**.\n",
    "\n",
    "Training configuration highlights:\n",
    "- **`per_device_train_batch_size=5`**: small batch size to fit a 7B model in memory.\n",
    "- **`gradient_accumulation_steps=4`**: simulates an effective batch size of 20.\n",
    "- **`num_train_epochs=1`**: a single pass over the train split for this demo.\n",
    "- **`learning_rate=3e-4`**: a typical LoRA fine-tuning learning rate.\n",
    "- **`fp16=True`** and **`optim=\"paged_adamw_8bit\"`**: use mixed precision and 8-bit Adam for efficiency.\n",
    "- **`evaluation_strategy=\"epoch\"`** and `eval_dataset`: run formal evaluation (eval loss) at the end of each epoch.\n",
    "\n",
    "The `Trainer` handles the training loop and evaluation, moving data to the correct device and updating **only the LoRA parameters** (base model weights remain frozen).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1,   # smaller per-device batch to fit 7B on Colab\n",
    "    per_device_eval_batch_size=1,    # smaller eval batch to avoid OOM during eval\n",
    "    gradient_accumulation_steps=16,  # keep a decent effective batch size\n",
    "    num_train_epochs=2,              # fewer epochs to keep runtime and memory manageable\n",
    "    learning_rate=1e-4,              # stable LR for LoRA/QLoRA on 7B\n",
    "    weight_decay=0.01,               # light regularization to help generalization\n",
    "    warmup_ratio=0.1,                # warmup to avoid early instability\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,     # save memory at the cost of some compute\n",
    "    logging_steps=1,\n",
    "    logging_first_step=True,\n",
    "    evaluation_strategy=\"steps\",    # run evaluation every N steps\n",
    "    eval_steps=25,                   # evaluate less frequently to reduce overhead\n",
    "    optim=\"paged_adamw_8bit\",      # 8-bit Adam optimizer (bitsandbytes)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training done.\")\n",
    "\n",
    "# Run a final evaluation pass to see eval loss\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Eval metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44fa780",
   "metadata": {},
   "source": [
    "### 6. Evaluate: Compare Before vs After Fine-Tuning\n",
    "\n",
    "We now evaluate the fine-tuned model on our **financial QA examples**.\n",
    "\n",
    "Steps:\n",
    "- Ask the **same question** we used before training.\n",
    "- Compare the **base model answer (before fine-tuning)** to the **fine-tuned model answer (after fine-tuning)**.\n",
    "- Query a couple of other questions from the dataset to see how well the model reproduces the ground-truth answers.\n",
    "\n",
    "We expect that after fine-tuning, the model will respond with answers that are **much closer to the dataset answers** for these examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060041d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model in eval mode and generate after fine-tuning\n",
    "model.eval()\n",
    "\n",
    "# Reuse the evaluation example from earlier\n",
    "prompt = f\"Question: {eval_question}\\nAnswer:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "\n",
    "ft_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Question:\", eval_question)\n",
    "print(\"Base model (before fine-tuning) answer:\\n\", base_answer)\n",
    "print(\"Fine-tuned model answer:\\n\", ft_answer)\n",
    "\n",
    "# Test a couple of other questions from the dataset as well\n",
    "test_indices = [1, 2]\n",
    "\n",
    "for idx in test_indices:\n",
    "    ex = dataset[idx]\n",
    "    q = ex[\"question\"]\n",
    "    gold = ex[\"answer\"]\n",
    "\n",
    "    prompt = f\"Question: {q}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the part after \"Answer:\" for readability\n",
    "    pred_answer = pred.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    print(f\"\\nQ: {q}\\nGold answer: {gold}\\nModel answer: {pred_answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ddf73",
   "metadata": {},
   "source": [
    "### Discussion and Next Steps\n",
    "\n",
    "In this hands-on tutorial, we:\n",
    "- Loaded a **7B open-source LLM** (Mistral 7B) in **4-bit (QLoRA) mode**.\n",
    "- Loaded the **OpenFinAL Financial_Question_Answering** dataset of financial questions and answers.\n",
    "- Formatted each example as a simple QA prompt (`Question: ...\\nAnswer: ...`).\n",
    "- Attached **LoRA adapters** to just a few attention submodules (`q_proj`, `v_proj`).\n",
    "- Fine-tuned **only the LoRA parameters** using the `Trainer` API.\n",
    "- Verified that the fine-tuned model **better reproduces the dataset answers** for sample financial questions.\n",
    "\n",
    "Possible extensions:\n",
    "- Add **train/validation** splits and compute metrics like BLEU or ROUGE.\n",
    "- Save and reload adapters with `PeftModel.save_pretrained` and `PeftModel.from_pretrained`.\n",
    "- Try **different prompt formats** (e.g., \"User:\" / \"Assistant:\" style chat prompts).\n",
    "- Experiment with **different LoRA ranks, learning rates, and training steps** to balance speed vs generalization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
